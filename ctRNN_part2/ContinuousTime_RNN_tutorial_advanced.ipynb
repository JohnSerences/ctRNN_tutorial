{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO_2jJncJEV9"
   },
   "source": [
    "# Continuous time recurrent neural network - more advanced topics\n",
    "\n",
    "john serences (jserences@ucsd.edu) for CSHL July 2024\n",
    "\n",
    "* In the `basics` tutorial we covered how to create and train a continuous time RNN. While some biological 'realism' was built in (e.g. activation function, pre/post activation noise), we did not place any constraints on the weight matrices, the degree to which units are connected, or the sign of the connections (i.e. excitatory vs inhibitory synapses). For example, in visual cortex we having excitatory neurons (pyramidal cells) and we have inhibitory interneurons (e.g. parvalbumin-expressing interneurons). Generally speaking these neurons serve a fixed function (Dale's principle), they exist in more-or-less known ratios, and they have different temporal properties. Implementing some of these constraints can dramaticallly change how an RNN solves a given problem, and also allows us to ask more targeted questions about the role of different cell types in information processing.\n",
    "* Dale's: unit j is excitatory if all of its projections on other units are zero or excitatory, i.e.,\n",
    "if $W^{hid}_{ij} > 0$ for all $i$ and inhibitory if $W^{hid}_{ij} < 0$ for all $i$ \n",
    "\n",
    "* We'll also cover some basic approaches for analyzing the state space of the RNNs after training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLyXB18kJEV_"
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wl4pad7bJEV_"
   },
   "outputs": [],
   "source": [
    "# minimal imports - can import sub modules to make code cleaner, but\n",
    "# keeping this simple for now\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# import class to make model inputs (defines different tasks, and can add other tasks easily using this framework)\n",
    "from tasks import ctTASKS\n",
    "\n",
    "# import class to make model, make hidden layer, and loss function\n",
    "from ct_rnn import *\n",
    "from h_layer import *\n",
    "\n",
    "# load the autoreload extension - don't need for tutorial but good if you ever modify the classes used in the tutorial \n",
    "# autoreload will update if you make changes to the ctTASKS class\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_layer = HLayer(rnn_settings)\n",
    "f_rt = torch.rand(64,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_layer = torch.nn.Linear(200,200)\n",
    "l_layer.weight = h_layer.weight\n",
    "l_layer.bias = h_layer.bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = l_layer(f_rt)\n",
    "f1 = h_layer(f_rt)\n",
    "f2 = torch.matmul(f_rt, torch.matmul(torch.relu(h_layer.weight), h_layer.mask)) + h_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(torch.isclose(f0,f1,atol=1e-04)!=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0[0,57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[0,57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inh1 = torch.where(f1[0,:]>0)[0]\n",
    "inh2 = torch.where(f2[0,:]>0)[0]\n",
    "plt.plot(f1[:,inh1].detach())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.diag(h_layer.mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_t = torch.rand((200,64))\n",
    "w = torch.rand((200,200))\n",
    "print(fr_t.shape)\n",
    "print(w.shape)\n",
    "bias = torch.rand(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.repeat(w[:, :, np.newaxis], 64, axis=2)\n",
    "print(w.shape)\n",
    "bias = np.repeat(bias[:, np.newaxis], 64, axis=1)\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = fr_t[:,10] * w[:,:,10] + bias[:,10]\n",
    "t2 = (fr_t * w + bias)[:,:,10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network params, task params, and build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'go-nogo' # task type (go-nogo, mante, xor)\n",
    "T = 100          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 25    # stim duration\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh =[0.8,0.2]  # to determine acc of model output: > acc_amp_thresh[0] is classified as a 'go' trial, < acc_amp_thresh[1] is 'nogo'\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delayed match to sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'dmts'    # task type (go-nogo, mante, dmts)\n",
    "T = 300          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 50    # stim duration\n",
    "delay = 10       # delay between stim1 and stim2\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh = 0.8  # to determine acc of model output: > acc_amp_thresh during target window is correct\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'delay' : delay, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mante 2013 task (response to sensory input depends on context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mante task has been initialized\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'mante'   # task type (go-nogo, mante, dmts)\n",
    "T = 500          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 200   # stim duration\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh =[0.8,0.2]  # to determine acc of model output: > acc_amp_thresh[0] is classified as a 'go' trial, < acc_amp_thresh[1] is 'nogo'\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the network and train model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QIPwV2x2JEWC",
    "outputId": "dca5a527-db37-484e-c373-411167c5f21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "Step 100, Loss 0.5041, Acc 0.0000\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "Step 200, Loss 0.5019, Acc 0.0000\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "Step 300, Loss 0.5005, Acc 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mmse_loss(outputs, targets)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# backprop the loss\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# single optimization step to update parameters\u001b[39;00m\n\u001b[1;32m    115\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \n",
      "File \u001b[0;32m~/pt/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pt/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pt/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# for debugging\n",
    "# torch.manual_seed(0)\n",
    "#--------------------------------\n",
    "# RNN Params\n",
    "#--------------------------------\n",
    "h_size = 200          # number of units in hidden layer\n",
    "inp_size = 4          # number of input streams (i.e. '1' means one stimulus input over time on each trial)[1 for go-ng0, 2 for dmts, 4 for mante]\n",
    "out_size = 1          # number of output streams\n",
    "dt = 1                # timestep\n",
    "tau = 20              # dt/tau determines decay time (how long prior state of a unit impacts current state)\n",
    "act_func = 'sigmoid'  # activation function linking v and fr for each unit (currently restricted to relu or sigmoid, but easy to add custom funcs)\n",
    "p_rec = 0.2           # probability of two hidden layer units forming a synapse\n",
    "p_inh = 0.2           # probability that a hidden layer connection, if formed, will be inhibitory\n",
    "apply_dale = True     # apply Dale's principle (i.e. exc and inh connections cannot change signs during training)\n",
    "w_dist = 'gaus'       # hidden layer weight distribution (gauss or gamma)\n",
    "w_gain = 1.5          # gain on weights in hidden layer if w_dist == gauss\n",
    "preact_n = 0.01       # noise applied before passing v through activation function to get fr\n",
    "postact_n = 0.01      # noise applied after passing v through activation function to get fr\n",
    "loss_crit = 0.01      # stop training if loss < loss_crit \n",
    "acc_crit = 0.98       # or stop training if prediction acc is > acc_crit\n",
    "\n",
    "# Each layer does y = xw + b so these \n",
    "# flags for determining which weights/biases\n",
    "# are trainable. \n",
    "W_in_trainable = False\n",
    "bias_in_trainable = False\n",
    "W_hid_trainable = True\n",
    "bias_hid_trainable = False\n",
    "W_out_trainable = True\n",
    "bias_out_trainable = True\n",
    "\n",
    "# Initial values of weights/biases for input and output layers and the bias \n",
    "# for the hidden layer...\n",
    "# We will setup the hidden unit weights in the RLayer class based on \n",
    "# the probability of excitatory and inhibitory connections and Dale's \n",
    "# principle (set above)\n",
    "W_in = torch.randn((h_size,inp_size),dtype=torch.float32)\n",
    "bias_in = torch.zeros(h_size,dtype=torch.float32)\n",
    "bias_hid = torch.zeros(h_size,dtype=torch.float32)\n",
    "W_out = torch.randn((out_size, h_size),dtype=torch.float32)/100\n",
    "bias_out = torch.zeros(out_size,dtype=torch.float32)\n",
    "\n",
    "# dict of params to init the network\n",
    "rnn_settings = {'h_size' : h_size, 'inp_size' : inp_size, 'out_size' : out_size,'dt' : dt, 'tau' : tau,  \n",
    "                'act_func' : act_func, 'p_rec' : p_rec, 'p_inh' : p_inh, \n",
    "                'apply_dale' : apply_dale, 'w_dist' : w_dist, 'w_gain' : w_gain, 'preact_n' : preact_n, 'postact_n' : postact_n, \n",
    "                'W_in_trainable' : W_in_trainable, 'bias_in_trainable' : bias_in_trainable, \n",
    "                'W_hid_trainable' : W_hid_trainable, 'bias_hid_trainable' : bias_hid_trainable,\n",
    "                'W_out_trainable' : W_out_trainable, 'bias_out_trainable' : bias_out_trainable, \n",
    "                'W_in' : W_in, 'bias_in' : bias_in, 'W_out' : W_out, 'bias_out' : bias_out}\n",
    "\n",
    "# Init the network object\n",
    "net = ctRNN(rnn_settings)\n",
    "\n",
    "#--------------------------------\n",
    "# Model training params\n",
    "#--------------------------------\n",
    "iters = 10000              # number of training iterations\n",
    "loss_update_step = 100  # output the current loss/acuracy every loss_update_step training iterations\n",
    "\n",
    "# learning rate of optimizer function - step size during gradient descent\n",
    "# if this is too large you might jump over minima and never converge, but \n",
    "# if this is too small then it can take a long time to find the minima\n",
    "learning_rate = 0.001   \n",
    "\n",
    "#--------------------------------\n",
    "# Train the model!\n",
    "#--------------------------------\n",
    "\n",
    "# Use Adam optimizer: Adaptive Moment Estimation (ADAM) combines\n",
    "# gradient descent with momentum (accelerates as approaching minima) \n",
    "# and RMSprop which uses a moving average of the squares of gradients\n",
    "# to adjust the learning rate for each weight in the model\n",
    "# to discourage exploration in directions with steep gradients\n",
    "# and to promote faster exploration of flatter regions. \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# for storing running average of loss and accuracy computed \n",
    "# over the last batch of trials every loss_update_step trials\n",
    "running_loss = 0\n",
    "running_acc = 0 \n",
    "\n",
    "# loop over number of model training iterations\n",
    "for i in range(iters):\n",
    "    \n",
    "    # get a batch of inputs and targets\n",
    "    if task_type == 'go-nogo':\n",
    "       inputs,tri_type = task.stim_go_nogo()  \n",
    "       targets = task.target_go_nogo(tri_type)\n",
    "    elif task_type == 'dmts':\n",
    "        inputs,tri_type = task.stim_dmts()  \n",
    "        targets = task.target_dmts(tri_type)   \n",
    "    elif task_type == 'mante':\n",
    "        inputs,tri_type = task.stim_mante()  \n",
    "        targets = task.target_mante(tri_type) \n",
    "    \n",
    "    # zero out the gradient buffers before updating model params (e.g. Weights/biases)\n",
    "    # because gradients accumulate so the new gradient will be \n",
    "    # combined with the old gradient which has already been used to update the model \n",
    "    # parameters and the combined gradient may point in the wrong direction (i.e. not towards the minima)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # pass inputs...get outputs\n",
    "    outputs, _ = net(inputs)\n",
    "\n",
    "    # compute loss given current output and target \n",
    "    # output on each trial in this batch\n",
    "    loss = net.mse_loss(outputs, targets)\n",
    "\n",
    "    # backprop the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # single optimization step to update parameters\n",
    "    optimizer.step()        \n",
    "\n",
    "    # update running loss (just to keep track and to primt out)\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Compute prediction accuracy (defined by the thresholds specified in settings dict)\n",
    "    \n",
    "    if (task_type == 'go-nogo') | (task_type == 'dmts'):\n",
    "        running_acc += task.compute_acc(settings,outputs,targets)    \n",
    "    else:\n",
    "        running_acc = 0\n",
    "        \n",
    "    # update about current loss and acc rate of model \n",
    "    # every loss_update_step steps\n",
    "    if i % loss_update_step == loss_update_step-1:\n",
    "        \n",
    "        print(net.recurrent_layer.h_layer.weight[0,:10]>0)\n",
    "        #print(net.output_layer.weight)\n",
    "\n",
    "        # compute avg loss and avg acc over last loss_update_step iterations\n",
    "        running_loss /= loss_update_step\n",
    "        running_acc /= loss_update_step\n",
    "        \n",
    "        # print out to monitor training\n",
    "        print(f'Step {i+1}, Loss {running_loss:0.4f}, Acc {running_acc:0.4f}')\n",
    "\n",
    "        # see if we've reached criteria to stop training\n",
    "        if (running_loss < loss_crit) | (running_acc > acc_crit):\n",
    "            print('Training finished')\n",
    "            break\n",
    "\n",
    "        # reset to zero before evaluating the loss and acc\n",
    "        # of the next loss_update_step iterations...\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 64, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.rand(64,2)\n",
    "\n",
    "il = torch.nn.Linear(2,200)\n",
    "il.weight = torch.nn.Parameter(W_in,requires_grad = W_in_trainable)\n",
    "#il.bias = torch.nn.Parameter((1,bias_in),requires_grad = bias_in_trainable)\n",
    "print(il.bias.shape)\n",
    "il(u)\n",
    "\n",
    "# w = net.recurrent_layer.inp_layer.weight\n",
    "# torch.matmul(u,w.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot output and targets of last batch to see how well the model is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the output of last batch (last group of trials)\n",
    "# to visualize model performance\n",
    "plt.plot(np.squeeze(outputs.detach().numpy()))\n",
    "plt.plot(np.squeeze(targets.detach().numpy()),'.')\n",
    "plt.title('Model output (solid), Targets (dots)')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at named model parameters to see current values of weights and biases and to see if they are trainable (requires_grad = True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.named_parameters():\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
